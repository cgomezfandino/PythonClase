{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDF5 with PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTables supports working with hierarchical data using Hierachical Data Format 5 (HDF5). It uses the binary HDF5 format that is designed to work with very large amounts of data and maintained by the <a href=\"http://www.hdfgroup.org/\">HDF Group</a> non-profit organization.\n",
    "\n",
    "HDF5 is widely used in various scientific domains and specially in supercomputing applications, due to its portable platforms, flexibility to support various data types and formats and support for efficient reading of data.\n",
    "\n",
    "In general, binary formats are more performant than text formats since they are more space efficient, and do not have the burden of converting from text to other types as numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About PyTables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.pytables.org/moin\">PyTables</a> is a library for handling HDF5 data in Python built on top of the HDF5 C libraries and NumPy. It provides also other additional improvements for performance in evaluating expressions and compression.\n",
    "\n",
    "Basic dataset classes include:\n",
    "* Arrays. Fixed amount of elements.\n",
    "* CArray (chunked array). For compression.\n",
    "* EArray (extendible array). Capable of extending elements.\n",
    "* VLArray (variable lenght array). The elements can be heterogeneous. \n",
    "* Tables (structured array with named fields). \n",
    "\n",
    "Basic data types:\n",
    "* bool, 8 bits.\n",
    "* int: 8, 16, 32 (default), 64.\n",
    "* uint: unsigned integers, 8, 16, 32 (default), 64.\n",
    "* float: 8, 16, 32 , 64 (default).\n",
    "* complex: 64 and 128 (default).\n",
    "* string: 8-bits.\n",
    "\n",
    "\n",
    "Documentation is here: http://pytables.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Line magic function `%rm` not found.\n",
      "ERROR:root:Line magic function `%rm` not found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " El volumen de la unidad C es Windows8_OS\n",
      " El nÂ£mero de serie del volumen es: A699-69CC\n",
      "\n",
      " Directorio de C:\\Users\\CarlosEduardo\\CiffBigData\\DataSciencePython\\bloque3\n",
      "\n",
      "10/11/2016  18:52    <DIR>          .\n",
      "10/11/2016  18:52    <DIR>          ..\n",
      "10/11/2016  18:34    <DIR>          .ipynb_checkpoints\n",
      "08/11/2016  18:18    <DIR>          __MACOSX\n",
      "08/11/2016  18:49            62.384 3_0_Reading tabular data with Pandas.ipynb\n",
      "08/11/2016  19:13            16.395 3_1_Alchemy with SQL.ipynb\n",
      "08/11/2016  20:16           118.196 3_2_Parsing XML with lxml.ipynb\n",
      "08/11/2016  20:39            12.730 3_3_A quick scraping example.ipynb\n",
      "08/11/2016  21:10            31.540 3_4_Getting data from Web APIs.ipynb\n",
      "10/11/2016  18:52            20.443 3_5_HDF5 with PyTables.ipynb\n",
      "08/11/2016  18:18           223.673 bloque3_guia.pdf\n",
      "08/11/2016  18:30           437.903 churn.txt\n",
      "08/11/2016  18:35               104 churn2.txt\n",
      "08/11/2016  19:03             2.048 customers.db\n",
      "08/11/2016  18:18         2.583.173 fa.xml\n",
      "08/11/2016  18:18            16.316 forbes-fragment.html\n",
      "08/11/2016  18:18         2.570.752 hdf5.PPT\n",
      "08/11/2016  18:18           152.584 NOOA_HDF5.ipynb\n",
      "08/11/2016  20:14            30.917 programs.jpg\n",
      "08/11/2016  18:18         1.568.256 sqlalchemy.PPT\n",
      "10/11/2016  18:43             2.240 temp.h5\n",
      "10/11/2016  18:54            74.140 temp2.h5\n",
      "              18 archivos      7.923.794 bytes\n",
      "               4 dirs  312.570.679.296 bytes libres\n"
     ]
    }
   ],
   "source": [
    "# cleaning:\n",
    "%ls\n",
    "%rm temp.h5\n",
    "%rm temp2.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp.h5 (File) 'My title of the file.'\n",
      "Last modif.: 'Thu Nov 10 18:54:55 2016'\n",
      "Object Tree: \n",
      "/ (RootGroup) 'My title of the file.'\n",
      "/my_first_table (Table(0,)) ''\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tables as tb\n",
    "\n",
    "# Open for write (from scratch):\n",
    "# You can also use r (read only), a(append) or \"r+\" (append but file must exist)\n",
    "f = tb.open_file(\"temp.h5\", \"w\", title=\"My title of the file.\")\n",
    "\n",
    "from tables import IsDescription, StringCol, Int64Col, UInt16Col\n",
    "class User(IsDescription):\n",
    "     name      = StringCol(16)   # 16-character String\n",
    "     interest  = Int64Col()      # Signed 64-bit integer\n",
    "     visits    = UInt16Col()     # Unsigned short integer\n",
    "\n",
    "\n",
    "# Create a table in the root of the file.\n",
    "f.create_table(\"/\", \"my_first_table\", User) #crea la tabla donde empiece por /\n",
    "print f\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5 is a file system in a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups are like folders (directories). There are also soft and hard links like in operating systems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) 'Another HDF5 file.'\n"
     ]
    }
   ],
   "source": [
    "f = tb.open_file(\"temp2.h5\", \"w\", title=\"Another HDF5 file.\")\n",
    "\n",
    "print f.root # The root of the hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating new nodes must be done in the file handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ (RootGroup) 'Another HDF5 file.'\n",
      "/new_group (Group) 'A new group!'\n"
     ]
    }
   ],
   "source": [
    "f.create_group(\"/\", \"new_group\", \"A new group!\")\n",
    "f.create_group(\"/\", \"yet_another_group\", \"Another one.\")\n",
    "# Pythonic navigation in the hierarchy (\"natural naming\" like in lxml):\n",
    "print f.root\n",
    "print f.root.new_group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating datasets (tables and arrays typically) on the file handle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/new_group/some_data (Array(4L,)) ''\n"
     ]
    }
   ],
   "source": [
    "f.create_array(\"/new_group\", \"some_data\", [7, 24, 56, 78]) # se crea una array en esta parte, se crea la carpeta \n",
    "# \"/new_group\", con el nombre de \"some_data, y se coloca los datos [7, 24, 56, 78]\n",
    "print f.root.new_group.some_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain metadata and information on particular nodes of the hierarchy via Python attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "little\n",
      "python\n"
     ]
    }
   ],
   "source": [
    "print f.root.new_group.some_data.size_on_disk\n",
    "print f.root.new_group.some_data.byteorder\n",
    "print f.root.new_group.some_data.flavor\n",
    "# etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables need descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables are extendable arrays that have a structured or **record** data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/new_group/sales (Table(0,)) ''\n",
       "  description := {\n",
       "  \"country\": StringCol(itemsize=2, shape=(), dflt='', pos=0),\n",
       "  \"sales\": Int32Col(shape=(), dflt=0, pos=1)}\n",
       "  byteorder := 'little'\n",
       "  chunkshape := (10922,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = np.dtype( [('country', 'S2'), ('sales', int)  ]) #podemos ahora poner los tipos de datos de country y sales\n",
    "# s2 = string 2, int = integer\n",
    "sales = np.array([(\"es\", 200), (\"fr\", 300), (\"uk\", 500)], dtype=dt)\n",
    "f.create_table(\"/new_group\", \"sales\", dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/new_group/sales (Table(3,)) ''\n"
     ]
    }
   ],
   "source": [
    "f.root.new_group.sales.append(sales)\n",
    "print f.root.new_group.sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'country': StringCol(itemsize=2, shape=(), dflt='', pos=0), 'sales': Int32Col(shape=(), dflt=0, pos=1)}\n"
     ]
    }
   ],
   "source": [
    "print f.root.new_group.sales.coldescrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTales tries to read the data in its original format in Python. Particularly, it recovers NumPy arrays, and datasets can be manipulated as with NumPy (indexing, slicing, masking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "('fr', 300)\n"
     ]
    }
   ],
   "source": [
    "a = f.root.new_group.sales[:]\n",
    "print type(a)\n",
    "print f.root.new_group.sales[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remember to do this.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides a HDFStore library to read and write dataframes from and to HDF5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store = pd.HDFStore('churn.h5',mode='w')\n",
    "for chunk in pd.read_csv('churn.txt',chunksize=50):\n",
    "         store.append('churn',chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: churn.h5\n",
      "/churn            frame_table  (typ->appendable,nrows->3333,ncols->21,indexers->[index])\n",
      "[ (0L, [265.1, 45.07, 197.4, 16.78, 244.7, 11.01, 10.0, 2.7], [128L, 415L, 25L, 110L, 99L, 91L, 3L, 1L], ['KS', '382-4657', 'no', 'yes', 'False.'])\n",
      " (1L, [161.6, 27.47, 195.5, 16.62, 254.4, 11.45, 13.7, 3.7], [107L, 415L, 26L, 123L, 103L, 103L, 3L, 1L], ['OH', '371-7191', 'no', 'yes', 'False.'])\n",
      " (2L, [243.4, 41.38, 121.2, 10.3, 162.6, 7.32, 12.2, 3.29], [137L, 415L, 0L, 114L, 110L, 104L, 5L, 0L], ['NJ', '358-1921', 'no', 'no', 'False.'])\n",
      " (3L, [299.4, 50.9, 61.9, 5.26, 196.9, 8.86, 6.6, 1.78], [84L, 408L, 0L, 71L, 88L, 89L, 7L, 2L], ['OH', '375-9999', 'yes', 'no', 'False.'])\n",
      " (4L, [166.7, 28.34, 148.3, 12.61, 186.9, 8.41, 10.1, 2.73], [75L, 415L, 0L, 113L, 122L, 121L, 3L, 3L], ['OK', '330-6626', 'yes', 'no', 'False.'])]\n"
     ]
    }
   ],
   "source": [
    "print store\n",
    "print store.root.churn.table[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
